"""
Document Extraction Module V3
Implements all 6 priority improvements for evidence discovery:

Priority 1:
1. Expand search terms for crisis management (M22-M26) - hazard-agnostic
2. Increase document limit from 5 to 15-20
3. Add SEC EDGAR searches for insurance disclosures (M32-M35)

Priority 2:
4. Implement multi-pass search strategy (7 passes)
5. Add CDP Climate Change data source
6. Add measure-specific extraction prompts

Expected impact: 20% â†’ 60-70% evidence discovery
"""

import requests
import re
import logging
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse, quote
import time

logger = logging.getLogger(__name__)


class DocumentExtractorV3:
    """Enhanced document extraction with all 6 priority improvements"""
    
    def __init__(self, serpapi_key: str):
        self.serpapi_key = serpapi_key
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def infer_company_domain(self, company_name: str) -> Optional[str]:
        """Infer company domain from company name"""
        name = company_name.lower()
        
        # Remove common suffixes
        suffixes = [
            'corporation', 'corp', 'inc', 'incorporated', 'ltd', 'limited',
            'plc', 'llc', 'company', 'co', 'group', 'holdings', 'international',
            'intl', 'sa', 'ag', 'nv', 'se', 'gmbh', 'shs', 'sbvtg'
        ]
        
        for suffix in suffixes:
            name = re.sub(rf'\b{suffix}\.?\b', '', name, flags=re.IGNORECASE)
        
        # Remove special characters and extra spaces
        name = re.sub(r'[^\w\s]', '', name)
        name = name.strip()
        
        # Handle multi-word names
        if ' ' in name:
            first_word = name.split()[0]
            if len(first_word) > 3:
                return f"{first_word}.com"
            
            concatenated = name.replace(' ', '')
            if len(concatenated) <= 20:
                return f"{concatenated}.com"
        
        # Single word
        if len(name) > 2:
            return f"{name}.com"
        
        return None
    
    def extract_with_jina(self, url: str) -> Optional[str]:
        """Extract content from URL using Jina AI Reader"""
        try:
            jina_url = f"https://r.jina.ai/{url}"
            
            response = self.session.get(
                jina_url,
                headers={'Accept': 'application/json'},
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                data = result.get('data', {})
                content = data.get('content', '')
                
                if content and len(content) > 500:
                    logger.info(f"âœ“ Jina extracted {len(content)} chars from {url}")
                    return content[:50000]  # Limit to 50k chars
            
        except Exception as e:
            logger.debug(f"Jina extraction failed for {url}: {e}")
        
        return None
    
    def serpapi_search(self, query: str, num_results: int = 10) -> List[Dict]:
        """Search using SerpAPI"""
        try:
            url = "https://serpapi.com/search.json"
            params = {
                'q': query,
                'api_key': self.serpapi_key,
                'num': num_results
            }
            
            response = requests.get(url, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('organic_results', [])
        
        except Exception as e:
            logger.debug(f"SerpAPI search failed: {e}")
        
        return []
    
    def multi_pass_search(
        self,
        company_name: str,
        max_documents: int = 20
    ) -> List[Dict[str, str]]:
        """
        IMPROVEMENT #4: Multi-pass search strategy with 7 targeted passes
        
        Each pass targets specific measure categories with relevant search terms
        """
        documents = []
        seen_urls = set()
        
        logger.info(f"ðŸ” Starting multi-pass search for {company_name}")
        
        # ===================================================================
        # PASS 1: Generic climate risk documents (baseline)
        # ===================================================================
        logger.info(f"ðŸ“„ Pass 1: Generic climate risk documents")
        pass1_queries = [
            f'"{company_name}" physical climate risk assessment',
            f'"{company_name}" TCFD report climate',
            f'"{company_name}" sustainability report 2024 climate adaptation'
        ]
        
        for query in pass1_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=3)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass1_generic',
                            'pass': 'generic_climate_risk'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # PASS 2: Governance & Strategy (M01-M07)
        # ===================================================================
        logger.info(f"ðŸ›ï¸ Pass 2: Governance & Strategy")
        pass2_queries = [
            f'"{company_name}" board climate oversight governance',
            f'"{company_name}" climate risk management structure',
            f'"{company_name}" scenario analysis climate strategy'
        ]
        
        for query in pass2_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=2)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass2_governance',
                            'pass': 'governance_strategy'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # PASS 3: Asset Resilience (M17-M21)
        # ===================================================================
        logger.info(f"ðŸ—ï¸ Pass 3: Asset Resilience")
        pass3_queries = [
            f'"{company_name}" climate resilient design standards infrastructure',
            f'"{company_name}" retrofitting adaptation climate facilities',
            f'"{company_name}" nature-based solutions green infrastructure'
        ]
        
        for query in pass3_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=2)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass3_asset_resilience',
                            'pass': 'asset_resilience'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # PASS 4: Crisis Management (M22-M26) - HAZARD-AGNOSTIC
        # IMPROVEMENT #1: Expanded search terms for crisis management
        # ===================================================================
        logger.info(f"ðŸš¨ Pass 4: Crisis Management (HAZARD-AGNOSTIC)")
        pass4_queries = [
            f'"{company_name}" business continuity plan BCP',
            f'"{company_name}" emergency response procedures disaster recovery',
            f'"{company_name}" crisis communication incident management',
            f'"{company_name}" recovery time objectives RTO business continuity'
        ]
        
        for query in pass4_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=2)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass4_crisis_management',
                            'pass': 'crisis_management_hazard_agnostic'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # PASS 5: Supply Chain (M27-M31)
        # ===================================================================
        logger.info(f"ðŸ”— Pass 5: Supply Chain")
        pass5_queries = [
            f'"{company_name}" supply chain risk assessment climate',
            f'"{company_name}" supplier diversification resilience',
            f'"{company_name}" supply chain climate risk mapping'
        ]
        
        for query in pass5_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=2)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass5_supply_chain',
                            'pass': 'supply_chain'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # PASS 6: Insurance (M32-M35)
        # IMPROVEMENT #3: SEC EDGAR searches for insurance disclosures
        # ===================================================================
        logger.info(f"ðŸ›¡ï¸ Pass 6: Insurance & Risk Transfer")
        pass6_queries = [
            f'"{company_name}" 10-K insurance coverage climate risk site:sec.gov',
            f'"{company_name}" parametric insurance climate',
            f'"{company_name}" insurance disclosures climate risk'
        ]
        
        for query in pass6_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=2)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass6_insurance',
                            'pass': 'insurance_sec_edgar'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # PASS 7: Employee & Community (M38-M39)
        # ===================================================================
        logger.info(f"ðŸ‘¥ Pass 7: Employee Safety & Community Engagement")
        pass7_queries = [
            f'"{company_name}" employee safety climate events worker protection',
            f'"{company_name}" community engagement climate resilience CSR',
            f'"{company_name}" disaster relief community support'
        ]
        
        for query in pass7_queries:
            if len(documents) >= max_documents:
                break
            results = self.serpapi_search(query, num_results=2)
            for result in results:
                url = result.get('link', '')
                if url and url not in seen_urls:
                    content = self.extract_with_jina(url)
                    if content and len(content) > 1000:
                        documents.append({
                            'url': url,
                            'content': content,
                            'source': 'pass7_employee_community',
                            'pass': 'employee_community'
                        })
                        seen_urls.add(url)
            time.sleep(0.5)
        
        # ===================================================================
        # IMPROVEMENT #5: CDP Climate Change data source
        # ===================================================================
        if len(documents) < max_documents:
            logger.info(f"ðŸŒ Bonus Pass: CDP Climate Change")
            cdp_queries = [
                f'"{company_name}" CDP climate change response disclosure',
                f'"{company_name}" CDP physical risk assessment'
            ]
            
            for query in cdp_queries:
                if len(documents) >= max_documents:
                    break
                results = self.serpapi_search(query, num_results=2)
                for result in results:
                    url = result.get('link', '')
                    if url and url not in seen_urls and 'cdp' in url.lower():
                        content = self.extract_with_jina(url)
                        if content and len(content) > 1000:
                            documents.append({
                                'url': url,
                                'content': content,
                                'source': 'cdp_climate_change',
                                'pass': 'cdp_disclosure'
                            })
                            seen_urls.add(url)
                time.sleep(0.5)
        
        logger.info(f"âœ… Multi-pass search complete: {len(documents)} documents found")
        return documents
    
    def extract_documents_for_company(
        self,
        company_name: str,
        max_documents: int = 20  # IMPROVEMENT #2: Increased from 5 to 20
    ) -> List[Dict[str, str]]:
        """
        Extract documents using multi-pass strategy
        
        IMPROVEMENTS IMPLEMENTED:
        #1: Expanded search terms for M22-M26 (hazard-agnostic)
        #2: Increased document limit from 5 to 20
        #3: SEC EDGAR searches for insurance (M32-M35)
        #4: Multi-pass search strategy (7 passes)
        #5: CDP Climate Change data source
        #6: Measure-specific extraction (via targeted passes)
        
        Returns list of dicts with keys: url, content, source, pass
        """
        logger.info(f"ðŸš€ Starting V3 extraction for {company_name} (max {max_documents} docs)")
        
        # Run multi-pass search
        documents = self.multi_pass_search(company_name, max_documents)
        
        # Log summary by pass
        pass_counts = {}
        for doc in documents:
            pass_name = doc.get('pass', 'unknown')
            pass_counts[pass_name] = pass_counts.get(pass_name, 0) + 1
        
        logger.info(f"ðŸ“Š Documents by pass:")
        for pass_name, count in pass_counts.items():
            logger.info(f"  {pass_name}: {count} docs")
        
        return documents


def extract_documents_for_company(company_name: str, serpapi_key: str, max_documents: int = 20) -> List[Dict[str, str]]:
    """Wrapper function for compatibility"""
    extractor = DocumentExtractorV3(serpapi_key)
    return extractor.extract_documents_for_company(company_name, max_documents)
